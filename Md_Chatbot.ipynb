{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dobrevajovana/LangChain_RAG/blob/main/Md_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<font color='orange'>**Md files Chatbot ü§óüìÅ**</font>\n",
        "\n",
        "This notebook contains complete code for the development of a chatbot/QA system."
      ],
      "metadata": {
        "id": "Qhld33hywf-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<font color='orange'> ‚è≠ Change your Runtime</font>\n",
        "On the task bar select Runtime-->Change Runtime, then add any GPU version that you have available (it doesn't metter if you are using basic or Pro Colab)"
      ],
      "metadata": {
        "id": "v6hpwl0dyo6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<font color='orange'>Download .md files</font>\n",
        "First we will need to load the dataset, that contains all .md files. For easier integration, when others try to run the colab, I download the folder entirely locally. In case the number of files increases, it can be called directly from Google Drive by mounting with our drive."
      ],
      "metadata": {
        "id": "xNyGAvy7BnO7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIK1F3kQBhij"
      },
      "outputs": [],
      "source": [
        "!gdown [ID-OF-YOUR-FOLDER]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/folder.zip\""
      ],
      "metadata": {
        "id": "znC9-LDxaKvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<font color='orange'>Requirements</font>\n",
        "Download all packages that we need for running the code"
      ],
      "metadata": {
        "id": "f1DqoIWdCf3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install langchain\n",
        "!pip3 install ipykernel jupyter\n",
        "!pip3 install auto-gptq==0.2.2\n",
        "!pip install requests openai transformers faiss-gpu\n",
        "!pip install unstructured\n",
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "-aeQDIw8w5YJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<font color='orange'>Building the chatbot"
      ],
      "metadata": {
        "id": "Nq5tOqciDGSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores.faiss import FAISS\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from transformers import AutoTokenizer, pipeline, logging\n",
        "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFacePipeline\n"
      ],
      "metadata": {
        "id": "fwmB27V7DP2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##<font color='orange'>Loading & splitting the data</font>\n",
        "Since we are talking about .md files that are in a folder and they need to be loaded we have two options:\n",
        "1. st Option that showed great results: It is most optimal to do the same with an already available class from langchain **[DirectoryLoader](https://js.langchain.com/docs/api/document_loaders_fs_directory/classes/DirectoryLoader)**. Here we indicate the type of files we will process (.md), and also the loader class, ie in our case **[UnstructuredMarkdownLoader](https://python.langchain.com/docs/modules/data_connection/document_loaders/markdown)**. I use this particular loader because it is suitable for the file structure. Also with this tehnique we will have as many files as there are documents, but the problem here is the LLM model, which accepts a maximum of 512 tokens. Now we need to split each of these documents into so-called chunks (pages). I use the **[RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter)** class, which splits each document context into characters, where 4 characters are one token. I set each chunk with a size of 1000 characters, I do not go to the maximum number of 2048 characters because the splitting is not always precise. At the same time, in order not to lose the context, I set an overlap between every two chunks to be 100 characters.\n",
        "2. nd Option, not so good: is to use  **[MarkdownHeaderTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/markdown_header_metadata)** class for splitting the files by headers. I go through all the files in the directory and split them by the header, after that I add manually the source file and pages in the metadata. One more problem is the chunk sizes, this is handeled with checking the context. If the document has more then 1000 characters, I split the document into two pages."
      ],
      "metadata": {
        "id": "y0uRn9MqpejO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Select the type of loading & splitting and run the cell\n",
        "option = '2nd option' # @param [\"1st option\", \"2nd option\"]"
      ],
      "metadata": {
        "id": "MxUUDX8oCCFU",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits=[]\n",
        "if option == '1st option':\n",
        "  text_loader_kwargs={'mode': 'elements'}\n",
        "  loader = DirectoryLoader('/content/folder/', glob=\"**/*.md\", loader_cls=UnstructuredMarkdownLoader,loader_kwargs=text_loader_kwargs)\n",
        "  docs = loader.load()\n",
        "  # Char-level splits\n",
        "  chunk_size = 1000\n",
        "  chunk_overlap = 100\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
        "  )\n",
        "  # Split\n",
        "  splits = text_splitter.split_documents(docs)\n",
        "else:\n",
        "  splits=[]\n",
        "  for filename in os.listdir('/content/folder/'):\n",
        "      f = os.path.join('/content/folder/', filename)\n",
        "      markdown_document = open(f,'r',encoding='utf-8').read()\n",
        "      headers_to_split_on = [\n",
        "          (\"#\", \"Header 1\"),\n",
        "          (\"##\", \"Header 2\"),\n",
        "          (\"###\", \"Header 3\"),\n",
        "      ]\n",
        "\n",
        "      markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "      md_header_splits = markdown_splitter.split_text(markdown_document)\n",
        "      page_number=1\n",
        "      for split in md_header_splits:\n",
        "        split.metadata['filename']=filename\n",
        "        split.metadata['page_number']=page_number\n",
        "        page_number+=1\n",
        "        if len(split.page_content)>1000:\n",
        "          split1=split\n",
        "          split2=split\n",
        "          split1.page_content = split1.page_content[:1000]\n",
        "          split2.page_content = split2.page_content[1000:]\n",
        "          page_number+=1\n",
        "          split.metadata['page_number']=page_number\n",
        "          splits.append(split1)\n",
        "          splits.append(split2)\n",
        "        else:\n",
        "          splits.append(split)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HmyxSzEySHQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='orange'>Vectorization of the Documents context</font>\n",
        "In order for all texts to be processed to the LLM model, they must first be vectorized. For the embedding model I can use an OpenAI, but the problem here is that at a given moment, when we would like to make a shift to an OpenSource LLM model, the OpenAI embedding will have to be completely replaced with another one, and we will automatically have so much more work to do. While OpenSouce embedding models are also adaptable whether for Llama, OpenAI or any other LLM model available on HuggingFace. I chose BGE model because according to the [leaderboard](https://huggingface.co/spaces/mteb/leaderboard) published on HuggingFace it is the best, and at the same time it does not take up too much RAM."
      ],
      "metadata": {
        "id": "QhUQMQYr6-8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"BAAI/bge-base-en\"\n",
        "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
        "\n",
        "model_norm = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs={'device': 'cuda'},\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n"
      ],
      "metadata": {
        "id": "W_KRRLsZqPoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a faster search of the documents, I will use [FAISS](https://python.langchain.com/docs/integrations/vectorstores/faiss). If the database of documents increases, FAISS can be replaced with a specific database, but for now it is our best option."
      ],
      "metadata": {
        "id": "4KfLSTBt9JJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db = FAISS.from_documents(splits, model_norm)"
      ],
      "metadata": {
        "id": "WTDSrCVgqzbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##<font color='orange'>LLM Model ü§ó</font>\n",
        "For this task I decided to use a quantized model of Llama-2-13B-chat. Why did I choose this model?\n",
        "\n",
        "1. If I load the model locally without quantization, there is no way I will have enough disk space for it\n",
        "2. In case I use HuggingFaceHub, which means directly sending requests to huggingface without downloading the model locally, it takes a very long period of time to get a response, and also after a while you need to subscribe because you run out of free tokens.\n",
        "3. I did not use a model from OpenAI because also I need subscription, and charging based on the number of tokens, and if this comes to production, the solution is not very scalable from a financial point of view.\n",
        "\n",
        "With the model TheBloke/Llama-2-13B-chat-GPTQ we get:\n",
        "1. Good performance\n",
        "2. Free OpenSource model"
      ],
      "metadata": {
        "id": "b2Ya9h269oL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title If you want to load model from OpenAI or use HuggingFace Hub here is the code you will need\n",
        "# @markdown if you want to run this code please copy and execute it in a code block.\n",
        "model = \"OpenAI\" #@param [\"OpenAI\",\"Llama-HuggingFaceHub\"]\n",
        "if model=='OpenAI':\n",
        "  print(\"\"\"\n",
        "  import os\n",
        "  from langchain.llms import OpenAI\n",
        "  os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "  llm = OpenAI()\n",
        "  \"\"\")\n",
        "else:\n",
        "  print( \"\"\"\n",
        "import os\n",
        "from langchain import HuggingFaceHub\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_KEY\n",
        "llm = HuggingFaceHub(repo_id=\"meta-llama/Llama-2-70b-chat-hf\", model_kwargs={\"temperature\":0.0, \"max_length\":512})\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZmJqfnygIgh",
        "outputId": "59f975cc-75bb-4da7-b9c9-50051b489051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  import os\n",
            "  from langchain.llms import OpenAI\n",
            "  os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
            "  llm = OpenAI()\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As I mentioned here will be used OpenSource quantized LLama model, so it can be locally downloaded and used without any payments."
      ],
      "metadata": {
        "id": "TVWgEYcXia9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
        "model_basename = \"model\"\n",
        "\n",
        "use_triton = False\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "\n",
        "\"\"\"\n",
        "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
        "        model_basename=model_basename,\n",
        "        use_safetensors=True,\n",
        "        trust_remote_code=True,\n",
        "        device=\"cuda:0\",\n",
        "        use_triton=use_triton,\n",
        "        quantize_config=None)\n",
        "\n",
        "\"\"\"\n",
        "#To download from a specific branch, use the revision parameter, as in this example:\n",
        "\n",
        "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
        "        revision=\"gptq-8bit-64g-actorder_True\",\n",
        "        model_basename=model_basename,\n",
        "        use_safetensors=True,\n",
        "        trust_remote_code=True,\n",
        "        device=\"cuda:0\",\n",
        "        quantize_config=None,\n",
        "        load_in_8bit=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "vptmwUJfzNyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Constructing custom prompt template:"
      ],
      "metadata": {
        "id": "uG-oViA8A2wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is missing for your solution to be production-ready?\"\n",
        "template=f'''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
        "USER: {prompt}\n",
        "ASSISTANT:\n",
        "'''\n",
        "\n",
        "# Prevent printing spurious transformers error when using pipeline with AutoGPTQ\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "print(\"*** Pipeline:\")\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.15\n",
        ")\n",
        "\n",
        "# print(pipe(template)[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJFT4jt5zNz8",
        "outputId": "d8f2edae-dd6d-4876-ca9f-ca1c4adc6f5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Pipeline:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When employing the Language Model (LM) to conduct document searches in response to specific queries, it is essential to initiate the process by specifying the number of documents to be included for consideration. This parameter is denoted as 'k' within the retriever component."
      ],
      "metadata": {
        "id": "HcDPK13MBFbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Select how many documents to be taken in the consideration for searching the answer\n",
        "k = \"5\" #@param [\"1\",\"2\",\"3\",\"4\",\"5\"]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Ax-hhd5fjlLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "conversation_chain = RetrievalQA.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=db.as_retriever(search_kwargs={\"k\": int(k)}),\n",
        "        return_source_documents=True,\n",
        "    )"
      ],
      "metadata": {
        "id": "ftOSvQADKqwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Formatting the output"
      ],
      "metadata": {
        "id": "4C0h0Du9BSx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def wrap_text_preserve_newlines(text, width=110):\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text\n",
        "\n",
        "def llm_chatbot(question):\n",
        "    llm_response=conversation_chain(question)\n",
        "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
        "    print('\\n\\nSources:')\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        print(source.metadata['filename'])"
      ],
      "metadata": {
        "id": "KsgeFtm0ggGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<font color='Orange'>Ask the bot about your md files"
      ],
      "metadata": {
        "id": "F9Z-aEpABfUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chatbot(\"your query\")"
      ],
      "metadata": {
        "id": "uO9menzhgm6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lVA7sMThdFC3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}